<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Body Posture Correction and Speech to Text</title>

<script
src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/0.7.3/p5.min.js"
type="text/javascript"
></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/0.7.3/addons/p5.dom.min.js"></script>
<script src="https://unpkg.com/ml5@0.2.3/dist/ml5.min.js"></script>
<script  >
  let videofeed;
let posenet;
let poses = [];
let started = false;
var audio = document.getElementById("audioElement");

// p5.js setup() function to set up the canvas for the web cam video stream
function setup() {
  //creating a canvas by giving the dimensions
  const canvas = createCanvas(600, 500);
  canvas.parent("video");

  videofeed = createCapture(VIDEO);
  videofeed.size(width, height);
  console.log("setup");

  // setting up the poseNet model to feed in the video feed.
  posenet = ml5.poseNet(videofeed);

  posenet.on("pose", function (results) {
    poses = results;
  });

  videofeed.hide();
  noLoop();
}

// p5.js draw function() is called after the setup function
function draw() {
  if (started) {
    image(videofeed, 0, 0, width, height);
    calEyes();
  }
}

// toggle button for starting the video feed
function start() {
  select("#startstop").html("Stop");
  document.getElementById("startstop").addEventListener("click", stop);
  started = true;
  loop();
}

// toggle button for ending the video feed
function stop() {
  select("#startstop").html("Start");
  document.getElementById("startstop").addEventListener("click", start);
  removeblur();
  started = false;
  loop();
}

// defining the parameters used for the posenet : the tracking of the eyes
var rightEye,
  leftEye,
  defaultRightEyePosition = [],
  defaultLeftEyePosition = [];

//function to calculate the position of the various keypoints
function calEyes() {
  for (let i = 0; i < poses.length; i++) {
    let pose = poses[i].pose;
    for (let j = 0; j < pose.keypoints.length; j++) {
      let keypoint = pose.keypoints[j];
      rightEye = pose.keypoints[2].position;
      leftEye = pose.keypoints[1].position;

      // keypoints are the points representing the different joints on the body recognized by posenet

      while (defaultRightEyePosition.length < 1) {
        defaultRightEyePosition.push(rightEye.y);
      }

      while (defaultLeftEyePosition.length < 1) {
        defaultLeftEyePosition.push(leftEye.y);
      }

      // if the current position of the body is too far from the original position blur function is called
      if (Math.abs(rightEye.y - defaultRightEyePosition[0]) > 20) {
        blur();
      }
      if (Math.abs(rightEye.y - defaultRightEyePosition[0]) < 20) {
        removeblur();
      }
    }
  }
}

//function to blur the background and add audio effect
function blur() {
  document.body.style.filter = "blur(5px)";
  document.body.style.transition = "1s";
  var audio = document.getElementById("audioElement");
  console.log("change");
  audio.play();
}

//function to remove the blur effect
function removeblur() {
  document.body.style.filter = "blur(0px)";
  var audio = document.getElementById("audioElement");

  audio.pause();
}

</script>

<style>

  *, *:after, *:before {
    -webkit-box-sizing: border-box;
    -moz-box-sizing: border-box;
    -ms-box-sizing: border-box;
    box-sizing: border-box;
}

body {
    font-family: Arial, sans-serif;
    font-size: 16px;
    margin: 0;
    display: flex;
    align-items: center;
    justify-content: center;
    min-height: 100vh;
    background: linear-gradient(to right bottom, #d13cff, #031f6a);
    color: #000;
}

.container {
    display: flex;
    justify-content: space-around;
    align-items: center;
    width: 90%;
    max-width: 1200px;
    margin: auto;
}

.left, .right {
    width: 45%;
}

.left {
    text-align: center;
}

.heading {
    color: #fff;
    font-size: 30px;
    padding: 15px;
}

.video {
    border-radius: 5px;
    margin-bottom: 20px;
}

.right {
    text-align: center;
}

.voice_to_text {
    width: 100%;
    max-width: 400px;
    margin: auto;
}

.voice_to_text h1 {
    color: #fff;
    font-size: 30px;
    margin-bottom: 20px;
}

#convert_text {
    width: 100%;
    height: 200px;
    border-radius: 10px;
    resize: none;
    padding: 10px;
    font-size: 20px;
    margin-bottom: 10px;
}

button {
    padding: 12px 20px;
    background: #0ea4da;
    border: 0;
    color: #fff;
    font-size: 18px;
    cursor: pointer;
    border-radius: 5px;
}

button:hover {
    background: #09a2d6;
}

#startstop {
	background: #ff0000;
}

</style>

</head>
<body>

<div class="container"> 
    <div class="left">
        <audio id="audioElement">
            <source src="" type="audio/wav" />
          </audio>
          
          <center>
          <h1 class="heading">Body Posture Correction</h1>
          <div id="video" class="video"></div>
            <button onClick="start()" id="startstop">
              Start
            </button>
      
          </center>
    </div>
    <div class="right">
      <div class="voice_to_text"> 
        <h1>Voice to Text Converter</h1>
        <textarea id="convert_text" rows="4" cols="50"></textarea>
        <button id="click_to_record">Start Recording</button>
        <button id="click_to_stop">Stop Recording</button>
    </div>
    </div>
</div>





</body>
<script>

  const convertText = document.getElementById("convert_text");
const startButton = document.getElementById("click_to_record");
const stopButton = document.getElementById("click_to_stop");
let recognition;
let lastTranscript = '';

startButton.addEventListener('click', function() {
    recognition = new window.webkitSpeechRecognition();
    recognition.interimResults = true;
    recognition.continuous = true; // Enable continuous recognition

    recognition.addEventListener('result', e => {
        const transcript = Array.from(e.results)
            .map(result => result[0])
            .map(result => result.transcript)
            .join('');

        if (transcript !== lastTranscript) {
            convertText.value = transcript; // Update textarea with new transcript
            lastTranscript = transcript; // Update lastTranscript
            console.log(transcript);
        }
    });

    recognition.start();
});


stopButton.addEventListener('click', function() {
    if (recognition) {
        recognition.stop();
    }
});

</script>
<script>
// Fetch JSON data using the fetch API
function loadData() {
  // Fetch JSON data using the fetch API
  fetch(' question.json')
    .then(function(response) {
      // Check if the request was successful
      if (!response.ok) {
        throw new Error('Failed to load JSON file');
      }
      
      // Parse JSON data
      return response.json();
    })
    .then(function(jsonData) {
      // Call function to process the data
      processData(jsonData);
    })
    .catch(function(error) {
      console.error(error);
    });
}

function processData(data) {
  // Process the JSON data here
  console.log('Received JSON data:', data);
  // Perform additional actions with the data
}

// Call the function to load data
loadData();


</script>
</html>
